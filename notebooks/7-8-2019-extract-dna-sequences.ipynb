{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pybedtools import BedTool\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import os\n",
    "import sys\n",
    "from shutil import copyfile\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = os.listdir('../data/norm_splice/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/encore_events'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-538dff149114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencore_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/encore_events'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencore_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HepG2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mrbp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/encore_events'"
     ]
    }
   ],
   "source": [
    "encore_dir = '../data/encore_events'\n",
    "for d in os.listdir(encore_dir):\n",
    "    if d.endswith('HepG2'):\n",
    "        rbp_name = d.split('-')[0]\n",
    "        \n",
    "        for event in events:\n",
    "            orig_file = '{}/{}/MATS_Norm_output/{}.MATS.JunctionCountOnly.txt'.format(encore_dir, d, event)\n",
    "            copy_file = '../data/norm_splice/{}/{}.MATS.JunctionCountOnly.txt'.format(event, rbp_name)\n",
    "            copyfile(orig_file, copy_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "`?*4 nt` sequence of exon-exon junction windows.\n",
    "\n",
    "`? nt` windows flanking the relevant exon/intron boundaries, extending a maximum of `? nt` into each exon and `? nt` into each intron.\n",
    "\n",
    "Feature windows:\n",
    "- upstream exon window\n",
    "- cassette exon 5' window\n",
    "- cassette exon 3' window\n",
    "- downstream exon window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(raw_filepath):\n",
    "    data = pd.read_csv('../data/norm_splice/SE/{}'.format(raw_filepath), sep='\\t')\n",
    "    bg_data = data.loc[data['FDR'] > 0.3]\n",
    "    bg_data = bg_data.iloc[np.random.randint(bg_data.shape[0], size=1)]\n",
    "    \n",
    "    data = data.loc[(data['FDR'] < 0.1) & (data['IncLevelDifference'] < 0)]\n",
    "    data = pd.concat([bg_data, data])\n",
    "    \n",
    "    rbp_name = raw_filepath.split('.')[0]\n",
    "    exon_overlap = 50\n",
    "    intron_overlap = 350\n",
    "\n",
    "    upstream_file = open(f'../data/features/SE/{rbp_name}.upstream.bed', 'w')\n",
    "    cassette_5p_file = open(f'../data/features/SE/{rbp_name}.cassette_5p.bed', 'w')\n",
    "    cassette_3p_file = open(f'../data/features/SE/{rbp_name}.cassette_3p.bed', 'w')\n",
    "    downstream_file = open(f'../data/features/SE/{rbp_name}.downstream.bed', 'w')\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        # upstream exon 3' region\n",
    "        upstream_3p_start = max(row['upstreamES'], row['upstreamEE'] - exon_overlap)\n",
    "        upstream_3p_end = min(row['exonStart_0base'], row['upstreamEE'] + intron_overlap)\n",
    "        upstream = [upstream_3p_start, upstream_3p_end]\n",
    "\n",
    "        # cassette exon 5' region\n",
    "        cassette_5p_start = max(row['upstreamEE'], row['exonStart_0base'] - intron_overlap)\n",
    "        cassette_5p_end = min(row['exonStart_0base'] + exon_overlap, row['exonEnd'])\n",
    "        cassette_5p = [cassette_5p_start, cassette_5p_end]\n",
    "\n",
    "        # cassette exon 3' region\n",
    "        cassette_3p_start = max(row['exonEnd'] - exon_overlap, row['exonStart_0base'])\n",
    "        cassette_3p_end = min(row['downstreamES'], row['exonEnd'] + intron_overlap)\n",
    "        cassette_3p = [cassette_3p_start, cassette_3p_end]\n",
    "\n",
    "        # downstream exon 5' window\n",
    "        downstream_5p_start = max(row['exonEnd'], row['downstreamES'] - intron_overlap)\n",
    "        downstream_5p_end = min(row['downstreamES'] + exon_overlap, row['downstreamEE'])\n",
    "        downstream = [downstream_5p_start, downstream_5p_end]\n",
    "\n",
    "        # Make sure windows are defined 5' -> 3'\n",
    "        assert (upstream[0] < upstream[1]) \n",
    "        assert (cassette_5p[0] < cassette_5p[1]) \n",
    "        assert (cassette_3p[0] < cassette_3p[1]) \n",
    "        assert (downstream[0] < downstream[1]) \n",
    "\n",
    "        chr = row['chr']\n",
    "\n",
    "        # write bed sequences\n",
    "        upstream_file.write(f'{chr}\\t{upstream[0]}\\t{upstream[1]}\\t{row[\"FDR\"]}\\n')\n",
    "        cassette_5p_file.write(f'{chr}\\t{cassette_5p[0]}\\t{cassette_5p[1]}\\t{row[\"FDR\"]}\\n')\n",
    "        cassette_3p_file.write(f'{chr}\\t{cassette_3p[0]}\\t{cassette_3p[1]}\\t{row[\"FDR\"]}\\n')\n",
    "        downstream_file.write(f'{chr}\\t{downstream[0]}\\t{downstream[1]}\\t{row[\"FDR\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c19b5a8dba49c4b64640e9381cd58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=223), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = Pool(8)\n",
    "\n",
    "se_files = os.listdir('../data/norm_splice/SE')\n",
    "total = len(se_files)\n",
    "\n",
    "for _ in tqdm_notebook(p.imap_unordered(generate_features, se_files), total=total):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate into 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde3ea7cc5a24ebb8b42407d855c2bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=893), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_dir = '../data/features/SE'\n",
    "output_suffix = ['upstream.bed', 'cassette_5p.bed', 'cassette_3p.bed', 'downstream.bed']\n",
    "\n",
    "for out_suffix in output_suffix:\n",
    "    out_feature_file = f'{feature_dir}_{out_suffix}'\n",
    "    if os.path.exists(out_feature_file):\n",
    "        os.remove(out_feature_file)\n",
    "    \n",
    "for file in tqdm_notebook(sorted(os.listdir(feature_dir))):\n",
    "    if file == '.ipynb_checkpoints':\n",
    "        continue\n",
    "        \n",
    "    filepath = os.path.join(feature_dir, file)\n",
    "    if os.path.getsize(filepath) > 1:\n",
    "        data = pd.read_csv(filepath, sep='\\t', header=None)\n",
    "\n",
    "        for out_suffix in output_suffix:\n",
    "            if filepath.endswith(out_suffix):\n",
    "                data[4] = file.split('.')[0]\n",
    "                \n",
    "#                 if file.split('.')[0] in keep:\n",
    "                \n",
    "                data.loc[data.iloc[:, 3] > 0.1, 4] = 'negative'\n",
    "                data = data.drop(axis=1, columns=3)\n",
    "                data.to_csv(f'{feature_dir}_{out_suffix}', sep='\\t', header=False, index=False, mode='a')        \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in all features as single matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for out_suffix in output_suffix:\n",
    "    if out_suffix == 'downstream.bed':\n",
    "        a.append(pd.read_csv('_'.join([feature_dir, out_suffix]), sep='\\t', header=None))\n",
    "    else:\n",
    "        a.append(pd.read_csv('_'.join([feature_dir, out_suffix]), sep='\\t', header=None).iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7187, 13)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = pd.concat(a, axis=1)\n",
    "events.columns = list(range(events.shape[1]))\n",
    "events.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = events.drop_duplicates(subset=events.columns[:-1], keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset events to most common classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "U2AF2       221\n",
       "negative    209\n",
       "U2AF1       173\n",
       "PABPC1       88\n",
       "HNRNPC       79\n",
       "SNRNP70      77\n",
       "MAGOH        72\n",
       "SRFBP1       64\n",
       "PUF60        58\n",
       "SRSF1        54\n",
       "SF3A3        53\n",
       "SRSF3        44\n",
       "RRP9         43\n",
       "EWSR1        42\n",
       "DDX19B       34\n",
       "RPL23A       34\n",
       "SF3B1        26\n",
       "KHSRP        25\n",
       "RAVER1       23\n",
       "HNRNPU       21\n",
       "UCHL5        20\n",
       "EIF4A3       19\n",
       "UBE2L3       17\n",
       "SRSF9        16\n",
       "SF3B4        15\n",
       "PRPF6        14\n",
       "EIF4G1       14\n",
       "HNRNPL       13\n",
       "RBM34        13\n",
       "FTO          13\n",
       "           ... \n",
       "TBRG4         1\n",
       "G3BP2         1\n",
       "RBM47         1\n",
       "ACO1          1\n",
       "PSIP1         1\n",
       "DNAJC2        1\n",
       "PCBP1         1\n",
       "CUGBP1        1\n",
       "NIP7          1\n",
       "AUH           1\n",
       "HNRNPA1       1\n",
       "TFIP11        1\n",
       "PPIG          1\n",
       "G3BP1         1\n",
       "RPS19         1\n",
       "RCC2          1\n",
       "CCAR1         1\n",
       "GRWD1         1\n",
       "ABCF1         1\n",
       "HNRNPUL1      1\n",
       "RBM27         1\n",
       "TARDBP        1\n",
       "XRN2          1\n",
       "PA2G4         1\n",
       "PKM2          1\n",
       "MATR3         1\n",
       "SUGP2         1\n",
       "RBFOX2        1\n",
       "PUS1          1\n",
       "PRPF4         1\n",
       "Name: 12, Length: 172, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[12].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "U2AF2       221\n",
       "negative    209\n",
       "U2AF1       173\n",
       "PABPC1       88\n",
       "HNRNPC       79\n",
       "SNRNP70      77\n",
       "MAGOH        72\n",
       "SRFBP1       64\n",
       "PUF60        58\n",
       "SRSF1        54\n",
       "SF3A3        53\n",
       "SRSF3        44\n",
       "RRP9         43\n",
       "EWSR1        42\n",
       "DDX19B       34\n",
       "RPL23A       34\n",
       "SF3B1        26\n",
       "KHSRP        25\n",
       "RAVER1       23\n",
       "HNRNPU       21\n",
       "UCHL5        20\n",
       "EIF4A3       19\n",
       "UBE2L3       17\n",
       "SRSF9        16\n",
       "SF3B4        15\n",
       "Name: 12, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep = events[12].value_counts()[:25].index\n",
    "# keep = keep.drop('negative') # remove negative controls\n",
    "events = events.loc[events[12].isin(keep)]\n",
    "events[12].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.shape[0] - (221 + 209+173)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write de-duped events to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [[0,1,2], [3,4,5], [6,7,8], [9,10,11]]\n",
    "feature_cols = [cols + [-1] for cols in feature_cols]\n",
    "\n",
    "for out_suffix, cols in zip(output_suffix, feature_cols):\n",
    "    out_feature_file = f'{feature_dir}_25class_dedup_{out_suffix}'\n",
    "\n",
    "    if os.path.exists(out_feature_file):\n",
    "        os.remove(out_feature_file)\n",
    "\n",
    "    events.iloc[:, cols].to_csv(out_feature_file, sep='\\t', header=False, index=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
